{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "def init_weight_variable(shape):\n",
    "    \n",
    "    sigma = 0.01\n",
    "    print \"initializing using sigma %f\" % sigma\n",
    "    initial = torch.zeros(shape)\n",
    "    initial.uniform_(-sigma, sigma)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        return nn.Parameter(initial.cuda())\n",
    "    else:\n",
    "        return nn.Parameter(initial)\n",
    "\n",
    "class MultiChannelCompressor(nn.Module):\n",
    "\n",
    "#     _TAU = 1.0\n",
    "#     _BATCH_SIZE = 64\n",
    "\n",
    "    def __init__(self, embedding, n_codebooks, n_centroids,_TAU=1.0,lr=0.0001,export_path=None,top_rate=0.05,num_pointers=1):\n",
    "        \n",
    "        \"\"\"\n",
    "        M: number of codebooks (subcodes)\n",
    "        K: number of vectors in each codebook\n",
    "        \"\"\"\n",
    "        \n",
    "        super(MultiChannelCompressor,self).__init__()\n",
    "        self.lr = lr\n",
    "        self.M = n_codebooks\n",
    "        self.K = n_centroids\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.softmax = nn.Softmax(-1)\n",
    "        self.V, self.embd_dim  = embedding.size()\n",
    "        self.export_path = export_path\n",
    "        self.embedding = nn.Embedding(self.V,self.embd_dim)\n",
    "        self.embedding.weight =nn.Parameter(Variable(embedding),requires_grad=False)\n",
    "#         self.bias =nn.Parameter(Variable(torch.rand([1,self.embd_dim])),requires_grad=True)\n",
    "\n",
    "        self.num_pointer = num_pointers\n",
    "\n",
    "        self.linear_h = nn.Linear(self.embd_dim, self.M*self.K/2)\n",
    "        self.linear_h.weight = init_weight_variable([self.M*self.K/2, self.embd_dim])\n",
    "        \n",
    "        self.linear_logit = nn.Linear(self.M*self.K/2, self.num_pointer * self.M *self.K)\n",
    "        self.linear_logit.weight = init_weight_variable([self.num_pointer * self.M * self.K,self.M*self.K/2])\n",
    "        \n",
    "        self.linear_mask_h = nn.Linear(self.embd_dim, self.M/2)\n",
    "        self.linear_mask_h.weight = init_weight_variable([self.M/2, self.embd_dim])\n",
    "        self.linear_mask_logit = nn.Linear(self.M/2, 2 * self.M)\n",
    "        self.linear_mask_logit.weight = init_weight_variable([2*self.M ,self.M/2])\n",
    "        self.num_top = int(self.V * top_rate)\n",
    "#         self.top_basis = nn.Parameter(embedding[:self.num_top] ,requires_grad=False)\n",
    "        \n",
    "        self.codebooks = init_weight_variable([ self.M * self.K, self.embd_dim])\n",
    "#         self.codebooks =  nn.Parameter(embedding[:self.M * self.K] /200,requires_grad=False)\n",
    "        \n",
    "        self.top_words_vecs = embedding[:self.num_top]\n",
    "        \n",
    "        self._TAU = _TAU\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.scale = nn.Parameter(Variable(torch.rand([1,self.num_pointer,self.embd_dim])),requires_grad=True)\n",
    "\n",
    "        parameters = [p for p in self.parameters() if p.requires_grad]\n",
    "        self.optimizer = torch.optim.Adam(parameters, lr=self.lr)\n",
    "        \n",
    "    def _gumbel_dist(self, shape, eps=1e-20):\n",
    "        U = torch.rand(shape).cuda()\n",
    "        return -torch.log(-torch.log(U + eps) + eps)\n",
    "\n",
    "    def _sample_gumbel_vectors(self, logits, temperature):\n",
    "        y = logits + self._gumbel_dist(logits.size())\n",
    "        return self.softmax( y / temperature)\n",
    "\n",
    "    def _gumbel_softmax(self, logits, temperature, sampling=True):\n",
    "        \"\"\"\n",
    "        Compute gumbel softmax.\n",
    "\n",
    "        \"\"\"\n",
    "        return self._sample_gumbel_vectors(logits, temperature)\n",
    "        \n",
    "    def _encode(self, word_ids,top_vecs):\n",
    "        input_embeds = self.embedding(word_ids)\n",
    "        num_words,dim = input_embeds.size()\n",
    "        M, K = self.M, self.K\n",
    "    \n",
    "        \n",
    "        h = self.tanh(self.linear_h(input_embeds))\n",
    "        \n",
    "        mask_h = self.tanh(self.linear_mask_h(input_embeds))\n",
    "        mask_logits = self.linear_mask_logit(mask_h)\n",
    "        mask_logits = torch.log(self.softplus(mask_logits) + 1e-8)\n",
    "        mask_logits = mask_logits.view([-1,  self.M ,2])\n",
    "        logits = self.linear_logit(h)\n",
    "        logits = torch.log(self.softplus(logits) + 1e-8)\n",
    "        logits = logits.view([-1,  self.num_pointer * M, K])\n",
    "    \n",
    "        return input_embeds, logits,mask_logits\n",
    "\n",
    "    def _decode(self, gumbel_output,top_out):\n",
    "        \n",
    "        return gumbel_output.mm(self.codebooks)\n",
    "    \n",
    "    def _reconstruct(self, codes, codebooks):\n",
    "        return None\n",
    "    \n",
    "    def var_loss(self,logits):\n",
    "        prob = self.softmax(logits-logits.max(-1,keepdim=True)[0])\n",
    "        prob = prob.view(-1,self.num_pointer, self.M, self.K)\n",
    "        prob = prob.transpose(1,2)\n",
    "        prob = prob.contiguous().view(-1,self.num_pointer,self.K)\n",
    "        prob = prob.bmm(prob.transpose(1,2))\n",
    "        b = torch.eye(self.num_pointer,self.num_pointer) \n",
    "        b = b.cuda()\n",
    "        loss_ =  ((prob - b [None,:,:]) ** 2).sum(-1).sum(-1)\n",
    "        loss_ = loss_.view(-1,self.M)\n",
    "        loss_ = loss_.sum(-1)\n",
    "        \n",
    "        return loss_\n",
    "    \n",
    "    def compress(self, word_ids,freq,top_vecs):\n",
    "        \"\"\"Export the graph for exporting codes and codebooks.\n",
    "\n",
    "        Args:\n",
    "            embed_matrix: numpy matrix of original embeddings\n",
    "        \"\"\"\n",
    "        vocab_size,embed_size = self.V,self.embd_dim\n",
    "        \n",
    "        # Coding\n",
    "        input_embeds, logits,mask_logits = self._encode(word_ids,top_vecs)  # ~ (B, M, K)\n",
    "#       \n",
    "        # Discretization\n",
    "        D = self._gumbel_softmax(logits, self._TAU, sampling=True)\n",
    "        D_mask = self._gumbel_softmax(mask_logits, self._TAU/2, sampling=True)\n",
    "        \n",
    "\n",
    "        D_mask = D_mask[:,:,0] #- D_mask[:,:,1]\n",
    "        \n",
    "        D = D.view(-1,self.num_pointer,self.M,self.K) * D_mask.view([-1,1,self.M,1])\n",
    "\n",
    "        gumbel_output = D.view([-1,  self.M * self.K])   # ~ (B, M * K)\n",
    "        \n",
    "        # Decoding\n",
    "        y_hat = self._decode(gumbel_output, D_mask)\n",
    "        y_hat = (self.sigmoid(self.scale) * y_hat.view(-1,self.num_pointer,embed_size)).sum(1) #+ self.bias\n",
    "        \n",
    "    \n",
    "        mask = (freq > 100 ).float()\n",
    "        \n",
    "        # Define loss\n",
    "        loss = 0.5   * ((y_hat - input_embeds)**2).sum(dim=1)\n",
    "        \n",
    "        loss_ =  0.5  * ((y_hat - input_embeds)**2).sum(dim=1)\n",
    "        loss_freq = (loss_ * mask).sum()\n",
    "        \n",
    "        loss = loss.mean()\n",
    "        \n",
    "#         loss += 0.05 * vloss.mean()\n",
    "\n",
    "#         loss = loss + 0.1 * loss_top\n",
    "        \n",
    "#         print loss_top \n",
    "        \n",
    "        return loss, y_hat - input_embeds, loss_freq\n",
    "\n",
    "\n",
    "    def compress_apply(self, word_ids, top_vecs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            word_ids to be encoded\n",
    "        \"\"\"\n",
    "        vocab_size, embed_size = [self.V,self.embd_dim]\n",
    "\n",
    "        batch_size = word_ids.size(0)\n",
    "        # Define codebooks\n",
    "        codebooks = self.codebooks\n",
    "        # Coding\n",
    "        input_embeds, logits,mask_logits = self._encode(word_ids,top_vecs)  # ~ (B, M, K)\n",
    "        \n",
    "        D_mask = (mask_logits[:,:,0] > mask_logits[:,:,1]).float()\n",
    "#         D_mask = D_mask.view(batch_size,self.num_pointer,self.M,self.K)\n",
    "        codes = logits.argmax(-1)\n",
    "        \n",
    "#         D_mask = torch.gather(D_mask,-1,codes_)\n",
    "        \n",
    "        \n",
    "        codes = codes.view(-1,self.num_pointer,self.M).view(-1,self.M)\n",
    "\n",
    "        # Reconstruct\n",
    "        offset = torch.arange(0,self.M).long().cuda() * self.K\n",
    "        codes_with_offset = codes + offset.view(1, self.M)\n",
    "        codes_with_offset = codes_with_offset.view(-1)\n",
    "        \n",
    "        selected_vectors = torch.index_select(codebooks,0,codes_with_offset)  # ~ (B, M, H)\n",
    "        selected_vectors = selected_vectors.view(batch_size,self.num_pointer,self.M,-1) * D_mask[:,None,:,None]       \n",
    "        selected_vectors = selected_vectors.sum(2)\n",
    "\n",
    "        reconstructed_embed =   (self.sigmoid(self.scale)* selected_vectors).sum(1)#+ self.bias#\n",
    "        \n",
    "        return reconstructed_embed.cpu().data.numpy(), input_embeds.cpu().data.numpy(),D_mask.cpu().data.numpy()\n",
    "    \n",
    "    \n",
    "    def _train(self,freq,sorted_index,batch_size=64,num_epochs=10):\n",
    "        \n",
    "        self.train()\n",
    "        vocab_size = self.V\n",
    "        top_reserve = self.num_top\n",
    "        word_list  = np.arange(0,vocab_size)\n",
    "\n",
    "        num_batches = (vocab_size) / batch_size \n",
    "        best_loss = 1000.\n",
    "        \n",
    "        if vocab_size % batch_size:\n",
    "            num_batches += 1\n",
    "            \n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            monitor_loss = 0.0\n",
    "            np.random.shuffle(word_list)\n",
    "            _freq = [freq[w] for w in word_list]\n",
    "            \n",
    "            freq_loss = 0\n",
    "            for i in range(num_batches):\n",
    "\n",
    "                words = word_list[i*batch_size:(i+1)*batch_size]\n",
    "                words = Variable(torch.LongTensor(words))\n",
    "                words = words.cuda()\n",
    "                freq_ = Variable(torch.Tensor(_freq[i*batch_size:(i+1)*batch_size])).cuda()\n",
    "                loss,diff,f_loss = self.compress(words,freq_,self.top_words_vecs)\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm(self.parameters(),0.001)\n",
    "                self.optimizer.step()\n",
    "                self.zero_grad()\n",
    "                monitor_loss += loss.data.cpu()\n",
    "                freq_loss += f_loss.data.cpu()\n",
    "            monitor_loss /= num_batches\n",
    "            monitor_loss = monitor_loss.data.cpu().numpy()\n",
    "            print \"epoch:\",epoch,\"loss:\",monitor_loss\n",
    "            if monitor_loss <= best_loss * 0.99:\n",
    "                best_loss = monitor_loss\n",
    "                self._test(sorted_index,64)\n",
    "                print \"best_loss:\", monitor_loss\n",
    "                \n",
    "    def _test(self,sorted_index, batch_size=64):\n",
    "        \n",
    "        self.eval()\n",
    "        vocab_size = self.V\n",
    "        top_reserve = self.num_top\n",
    "        word_list  = np.arange(0,vocab_size)\n",
    "        \n",
    "\n",
    "        \n",
    "        new_embedding = np.zeros([self.V,self.embd_dim],dtype=np.float32)\n",
    "        old_embedding = self.embedding.weight.data.cpu().numpy()\n",
    "\n",
    "        num_batches = (vocab_size) / batch_size \n",
    "\n",
    "        if vocab_size % batch_size:\n",
    "            num_batches += 1\n",
    "            \n",
    "        distance = []\n",
    "        reconstructed_ = []\n",
    "        top_words = set()\n",
    "        num_codes = 0\n",
    "        for i in range(num_batches):\n",
    "\n",
    "            words = word_list[i*batch_size:(i+1)*batch_size]\n",
    "            words = Variable(torch.LongTensor(words))\n",
    "            words = words.cuda()\n",
    "\n",
    "            reconstructed_vecs, original_vecs,activate_code = self.compress_apply(words,self.top_words_vecs)\n",
    "            num_codes +=  activate_code.sum()\n",
    "\n",
    "            reconstructed_.append(reconstructed_vecs)\n",
    "            distance.append(np.linalg.norm(reconstructed_vecs - original_vecs,2,-1))\n",
    "        reconstruct = np.concatenate(reconstructed_,0)\n",
    "        \n",
    "        distance = np.concatenate(distance,-1)\n",
    "        new_embedding[sorted_index] = reconstruct\n",
    "        cnt = 0 \n",
    "        \n",
    "        for i in range(top_reserve):\n",
    "            if distance[i] < 5.0:\n",
    "                cnt += 1\n",
    "            else:\n",
    "                new_embedding[sorted_index[i]] = self.top_words_vecs[i]\n",
    "#         new_embedding[sorted_index[:top_reserve]] = self.top_words_vecs\n",
    "\n",
    "        if self.export_path:\n",
    "            np.save(open(self.export_path,'w'),new_embedding)\n",
    "        print \"test:\",distance.mean()\n",
    "        print \"%d top words get compressed\" % (cnt) \n",
    "        compression = (top_reserve-cnt+self.num_pointer) * self.embd_dim * 16 + self.M*self.K*self.embd_dim * 16 + (self.V-top_reserve) * self.M + num_codes*self.num_pointer*math.log(self.K,2)\n",
    "        print \"\"\n",
    "        print \"compression_rate:\",(self.V * self.embd_dim * 32)/ compression\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing using sigma 0.010000\n",
      "initializing using sigma 0.010000\n",
      "initializing using sigma 0.010000\n",
      "initializing using sigma 0.010000\n",
      "initializing using sigma 0.010000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p27/lib/python2.7/site-packages/ipykernel/__main__.py:236: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 19.267584\n",
      "test: 6.299299\n",
      "0 top words get compressed\n",
      "\n",
      "compression_rate: 25.1247653975031\n",
      "best_loss: 19.267584\n",
      "epoch: 1 loss: 18.197708\n",
      "test: 6.232256\n",
      "0 top words get compressed\n",
      "\n",
      "compression_rate: 25.10642508985485\n",
      "best_loss: 18.197708\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9d557eaf8a01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-2306564531c0>\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, freq, sorted_index, batch_size, num_epochs)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda3/envs/pytorch_p27/lib/python2.7/site-packages/torch/nn/utils/clip_grad.pyc\u001b[0m in \u001b[0;36mclip_grad_norm\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \"\"\"\n\u001b[1;32m     49\u001b[0m     warnings.warn(\"torch.nn.utils.clip_grad_norm is now deprecated in favor \"\n\u001b[0;32m---> 50\u001b[0;31m                   \"of torch.nn.utils.clip_grad_norm_.\", stacklevel=2)\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "matrix_ = './data/W1.npy' #path to W1.npy\n",
    "mat = np.load(matrix_)\n",
    "def remap(matrix, freq):\n",
    "    index =  np.argsort(freq)[::-1]\n",
    "    return matrix[index],index,[freq[i] for i in index]\n",
    "import cPickle as pickle\n",
    "freq = np.load(\"./data/freqs\") #path to frequency\n",
    "\n",
    "freq = [freq[i] for i in range(len(mat))]\n",
    "mat_,index,freq = remap(mat,freq)\n",
    "matrix = torch.from_numpy(mat_)\n",
    "matrix = matrix\n",
    "\n",
    "M = 16\n",
    "K = 16\n",
    "\n",
    "model = MultiChannelCompressor(matrix,M,K,_TAU=1.0,lr=0.001\n",
    "                            ,export_path=\"./data/W1.shu.top_cut.npy\",top_rate=0.05,num_pointers=3) #output to data/W1.shu.top_cut.npy\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "model._train(freq,index,64,num_epochs=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p27)",
   "language": "python",
   "name": "conda_pytorch_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

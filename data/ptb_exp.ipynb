{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "is_training = True\n",
    "Py3 = sys.version_info[0] == 3\n",
    "CUDNN = \"cudnn\"\n",
    "num_steps = 35\n",
    "def _get_lstm_cell():\n",
    "    return tf.contrib.rnn.LSTMBlockCell(hidden_size, forget_bias=0.0)\n",
    "\n",
    "def make_cell():\n",
    "    cell = _get_lstm_cell()\n",
    "    return cell\n",
    "\n",
    "hidden_size = 1500\n",
    "batch_size = 20\n",
    "\n",
    "def _read_words(filename):\n",
    "  with tf.gfile.GFile(filename, \"r\") as f:\n",
    "    if Py3:\n",
    "      return f.read().replace(\"\\n\", \"<eos>\").split()\n",
    "    else:\n",
    "      return f.read().decode(\"utf-8\").replace(\"\\n\", \"<eos>\").split()\n",
    "\n",
    "def _build_vocab(filename):\n",
    "  data = _read_words(filename)\n",
    "\n",
    "  counter = collections.Counter(data)\n",
    "  count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "  words, _ = list(zip(*count_pairs))\n",
    "  word_to_id = dict(zip(words, range(len(words))))\n",
    "\n",
    "  return word_to_id\n",
    "\n",
    "def _file_to_word_ids(filename, word_to_id):\n",
    "  data = _read_words(filename)\n",
    "  return [word_to_id[word] for word in data if word in word_to_id]\n",
    "\n",
    "def ptb_raw_data(data_path=None):\n",
    "  \"\"\"Load PTB raw data from data directory \"data_path\".\n",
    "  Reads PTB text files, converts strings to integer ids,\n",
    "  and performs mini-batching of the inputs.\n",
    "  The PTB dataset comes from Tomas Mikolov's webpage:\n",
    "  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
    "  Args:\n",
    "    data_path: string path to the directory where simple-examples.tgz has\n",
    "      been extracted.\n",
    "  Returns:\n",
    "    tuple (train_data, valid_data, test_data, vocabulary)\n",
    "    where each of the data objects can be passed to PTBIterator.\n",
    "  \"\"\"\n",
    "\n",
    "  train_path = os.path.join(data_path, \"train.txt\")\n",
    "  valid_path = os.path.join(data_path, \"valid.txt\")\n",
    "  test_path = os.path.join(data_path, \"test.txt\")\n",
    "\n",
    "  word_to_id = _build_vocab(train_path)\n",
    "  train_data = _file_to_word_ids(train_path, word_to_id)\n",
    "  valid_data = _file_to_word_ids(valid_path, word_to_id)\n",
    "  test_data = _file_to_word_ids(test_path, word_to_id)\n",
    "  vocabulary = len(word_to_id)\n",
    "  return train_data, valid_data, test_data, vocabulary\n",
    "\n",
    "\n",
    "def data_type():\n",
    "  return tf.float32 if 1 else tf.float32\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import collections\n",
    "\n",
    "raw_data = ptb_raw_data(\"simple-examples/\")\n",
    "train_data, valid_data, test_data, _ = raw_data\n",
    "\n",
    "_input = tf.placeholder(tf.int32, shape=(batch_size, num_steps), name=\"input\")\n",
    "_targets = tf.placeholder(tf.int32, shape=(batch_size, num_steps), name=\"targets\")\n",
    "vocab_size = 10000\n",
    "keep_prob = 0.35\n",
    "num_layers = 2\n",
    "\n",
    "init_scale = 0.04\n",
    "initializer = tf.random_uniform_initializer(-init_scale,init_scale)\n",
    "max_epoch = 14\n",
    "lr_decay = 1/1.15\n",
    "lrlr = 1.\n",
    "\n",
    "with tf.variable_scope('MODEL', initializer=initializer,reuse=False):\n",
    "    learning_rate = tf.Variable(1., trainable=False)\n",
    "    embedding = tf.get_variable(\"embedding\", [vocab_size, hidden_size], dtype=data_type())\n",
    "    inputs = tf.nn.embedding_lookup(embedding, _input)\n",
    "    val_inputs = inputs\n",
    "    inputs = tf.nn.dropout(inputs, keep_prob)\n",
    "    val_cell = [make_cell() for _ in range(num_layers)]\n",
    "    cell = [tf.contrib.rnn.DropoutWrapper(c,output_keep_prob=keep_prob) for c in val_cell]\n",
    "    the_cell = tf.contrib.rnn.MultiRNNCell(cell, state_is_tuple=True)\n",
    "    #val_cell = tf.contrib.rnn.MultiRNNCell(val_cell)\n",
    "    initial_state = the_cell.zero_state(batch_size, data_type())\n",
    "    #inputs = tf.unstack(inputs, num=num_steps, axis=1)\n",
    "    #outputs, final_state = tf.nn.static_rnn(the_cell, inputs,initial_state=initial_state)\n",
    "    #cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(the_cell, inputs, initial_state=initial_state,dtype=data_type())\n",
    "    #val_outputs, val_final_state = tf.nn.dynamic_rnn(val_cell, val_inputs, initial_state=initial_state,dtype=tf.float32)\n",
    "    #val_output = tf.reshape(tf.concat(val_outputs, 1), [-1, hidden_size])\n",
    "    output = tf.reshape(tf.concat(outputs, 1), [-1, hidden_size])\n",
    "    softmax_w = tf.get_variable(\"softmax_W\", [hidden_size, vocab_size], dtype=data_type())\n",
    "    softmax_b = tf.get_variable(\"softmax_B\", [vocab_size], dtype=data_type())\n",
    "    logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)\n",
    "    logits = tf.reshape(logits, [batch_size, num_steps, vocab_size])\n",
    "    #val_logits = tf.nn.xw_plus_b(val_output, softmax_w, softmax_b)\n",
    "    #val_logits = tf.reshape(val_logits, [batch_size, num_steps, vocab_size])\n",
    "\n",
    "    loss = tf.contrib.seq2seq.sequence_loss(\n",
    "            logits,\n",
    "            _targets,\n",
    "            tf.ones([batch_size, num_steps], dtype=data_type()),\n",
    "            average_across_timesteps=False,\n",
    "            average_across_batch=True)\n",
    "    cost = tf.reduce_sum(loss)\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),10)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    new_lr = tf.placeholder(tf.float32, shape=[])\n",
    "    lr_update = tf.assign(learning_rate, new_lr)\n",
    "    \n",
    "with tf.variable_scope('MODEL', initializer=initializer,reuse=True):\n",
    "    val_cell = tf.contrib.rnn.MultiRNNCell(val_cell)\n",
    "    val_outputs, val_final_state = tf.nn.dynamic_rnn(val_cell, val_inputs, initial_state=initial_state,dtype=tf.float32)\n",
    "    val_output = tf.reshape(tf.concat(val_outputs, 1), [-1, hidden_size])\n",
    "    val_logits = tf.nn.xw_plus_b(val_output, softmax_w, softmax_b)\n",
    "    val_logits = tf.reshape(val_logits, [batch_size, num_steps, vocab_size])\n",
    "\n",
    "    val_loss = tf.contrib.seq2seq.sequence_loss(\n",
    "            val_logits,\n",
    "            _targets,\n",
    "            tf.ones([batch_size, num_steps], dtype=data_type()),\n",
    "            average_across_timesteps=False,\n",
    "            average_across_batch=True)\n",
    "    val_cost = tf.reduce_sum(val_loss)\n",
    "\n",
    "config = tf.ConfigProto(\n",
    "    gpu_options = tf.GPUOptions(allow_growth=True,visible_device_list=\"0\"),\n",
    "    device_count = {'GPU': 1},\n",
    "\n",
    "#    log_device_placement = True                                                                                     \\\n",
    "                                                                                                                      \n",
    ")\n",
    "sess = tf.Session(config = config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data_len = len(train_data)\n",
    "batch_len = data_len // num_steps\n",
    "num_batches = int(data_len / (batch_size * num_steps))\n",
    "xdata = train_data[:num_batches * batch_size * num_steps]\n",
    "ydata = np.copy(xdata)\n",
    "ydata[:-1] = xdata[1:]\n",
    "ydata[-1] = xdata[0]\n",
    "x_batches = np.split(np.asarray(xdata).reshape(batch_size, -1), num_batches, 1)\n",
    "y_batches = np.split(np.asarray(ydata).reshape(batch_size, -1), num_batches, 1)\n",
    "valid_len = len(valid_data)\n",
    "num_batches = int(valid_len / (batch_size * num_steps))\n",
    "vdata = valid_data[:num_batches * batch_size * num_steps]\n",
    "vydata = np.copy(vdata)\n",
    "vydata[:-1] = vdata[1:]\n",
    "vydata[-1] = vdata[0]\n",
    "x_val_batches = np.split(np.asarray(vdata).reshape(batch_size, -1), num_batches, 1)\n",
    "y_val_batches = np.split(np.asarray(vydata).reshape(batch_size, -1), num_batches, 1)\n",
    "\n",
    "\n",
    "test_len = len(test_data)\n",
    "num_batches = int(test_len / (batch_size * num_steps))\n",
    "tdata = test_data[:num_batches * batch_size * num_steps]\n",
    "tydata = np.copy(tdata)\n",
    "tydata[:-1] = tdata[1:]\n",
    "tydata[-1] = tdata[0]\n",
    "x_test_batches = np.split(np.asarray(tdata).reshape(batch_size, -1), num_batches, 1)\n",
    "y_test_batches = np.split(np.asarray(tydata).reshape(batch_size, -1), num_batches, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'Large_LSTM.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mIOError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-02ddba05c11f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mVars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Large_LSTM.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtvars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mW1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtvars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mW2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtvars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ec2-user/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/numpy/lib/npyio.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_pathlib_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'Large_LSTM.npy'"
     ]
    }
   ],
   "source": [
    "Vars = np.load(\"Large_LSTM.npy\")\n",
    "for k,v in enumerate(tvars):\n",
    "    sess.run(tf.assign(v,Vars[k]))\n",
    "W1 = sess.run(tvars[0])\n",
    "W2 = sess.run(tvars[-2])\n",
    "W2 = W2.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.32328562594896\n"
     ]
    }
   ],
   "source": [
    "# VALIDATE #\n",
    "stateb = sess.run(initial_state)\n",
    "t = np.arange(len(x_test_batches))\n",
    "cc = 0.0\n",
    "cnt = 0\n",
    "for k,idx in enumerate(t):\n",
    "    feed_dict = {}\n",
    "    for i, (c, h) in enumerate(initial_state):\n",
    "        feed_dict[c] = stateb[i].c\n",
    "        feed_dict[h] = stateb[i].h\n",
    "    feed_dict[_input] = x_test_batches[idx]\n",
    "    feed_dict[_targets] = y_test_batches[idx]    \n",
    "    _c,stateb = sess.run([val_cost,val_final_state],feed_dict=feed_dict)\n",
    "    cc += _c\n",
    "    cnt += num_steps\n",
    "\n",
    "print np.exp(cc/cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Compress(X,compression_rate, Weights, top_protected_ratio, n_clusters, k_recon_iter):\n",
    "    \"\"\"\n",
    "    X : Matrix to compress with shape vocab_size x hidden_size, assumed the rows are already sorted by\n",
    "        frequency of the word\n",
    "    compression_rate : scalar, compression rate of the matrix X\n",
    "    weights: weights of each vocaburary in X with shape 1xvocab_size\n",
    "    top_protected_ratio: scalr, percentage of vocaburaries not approximated at all\n",
    "    k_mean_iter: number of iterations of K-reconstruction process\n",
    "    return values:\n",
    "    \n",
    "    \"\"\"\n",
    "    def weighted_svd(A, weight, k):\n",
    "        # weight is for with each row of A \n",
    "        # solve min_{U, V} ||WUV^T - WA||^2,   W = diag(sqrt(weight))\n",
    "        W = np.diag(weight**(0.5))\n",
    "        QA = np.matmul(W,A)\n",
    "\n",
    "        m_square = np.dot(np.transpose(QA),QA)\n",
    "        u, s, v = np.linalg.svd(m_square, full_matrices=True)\n",
    "        v = np.transpose(v)\n",
    "        if(k > 0):\n",
    "            U = np.dot(np.diag(1/(weight**0.5)),np.dot(QA,v[:, 0:k]))\n",
    "            V = np.transpose(v[:, 0:k])\n",
    "            return U,V\n",
    "        else:\n",
    "            return v\n",
    "\n",
    "\n",
    "    ## Initial Setup\n",
    "    hidden_size = X.shape[1]\n",
    "    entries_to_take = int(X.shape[0] * top_protected_ratio)\n",
    "    X_sub = X[entries_to_take:,:]\n",
    "    Weight_sub = Weights[entries_to_take:]\n",
    "    target_rate = compression_rate / ( 1- top_protected_ratio)\n",
    "    total_rank = (np.prod(X_sub.shape) * target_rate)/(X_sub.shape[0] + hidden_size)\n",
    "    budget_rate = n_clusters * target_rate\n",
    "    # Create initial basis and assignment\n",
    "    vocab_size = X_sub.shape[0]\n",
    "    idx = np.arange(vocab_size)\n",
    "    initial_labels = np.asarray([np.random.randint(n_clusters) for p in range(vocab_size)])\n",
    "    chunk_size = int(float(X_sub.shape[0])/n_clusters)\n",
    "    for k in range(n_clusters):\n",
    "        initial_labels[k*chunk_size:(k+1)*chunk_size] = k\n",
    "    total_freq = float(sum(Weight_sub))\n",
    "    basis = []\n",
    "    ranks = []\n",
    "    budgets = []\n",
    "    for c in range(n_clusters):\n",
    "        vecs = X_sub[np.where(initial_labels == c)[0]]\n",
    "        weight_for_this_cluster = Weight_sub[np.where(initial_labels == c)[0]]\n",
    "        local_rate = budget_rate * sum(weight_for_this_cluster)/total_freq\n",
    "        rank_for_this_cluster = int(np.prod(vecs.shape) * local_rate / (vecs.shape[0] + hidden_size))\n",
    "        #rank_for_this_cluster = int(sum(weight_for_this_cluster)/total_freq * total_rank)\n",
    "        ranks.append(rank_for_this_cluster)\n",
    "        print rank_for_this_cluster\n",
    "        #rank_for_this_cluster = int(np.where(np.abs(vecs) > th)[0].shape[0]/(vecs.shape[0] + hidden_size))\n",
    "        #ranks.append(int(np.where(np.abs(vecs) > th)[0].shape[0]/(vecs.shape[0] + hidden_size)))\n",
    "    new_assignment = initial_labels\n",
    "    ranks_for_each_param= []\n",
    "    for i in range(X_sub.shape[0]):\n",
    "        ranks_for_each_param.append(ranks[new_assignment[i]])\n",
    "    minimal_rank = min(ranks)\n",
    "    maximal_rank = max(ranks)\n",
    "    for c in range(n_clusters):\n",
    "        vecs = X_sub[np.where(initial_labels == c)[0]]\n",
    "        weight_for_this_cluster = Weight_sub[np.where(initial_labels == c)[0]]\n",
    "        B = weighted_svd(vecs,np.log(weight_for_this_cluster)+1e-20,0)\n",
    "        basis.append(B)\n",
    "\n",
    "    # Iteratively assign to the cluster with best approximation\n",
    "    sample_order = np.arange(vocab_size)\n",
    "    for i in range(k_recon_iter):\n",
    "\n",
    "        recon_err = []\n",
    "\n",
    "        for cc,x in enumerate(X_sub):\n",
    "            projs = []\n",
    "            rr = ranks_for_each_param[cc]\n",
    "            for k,c in enumerate(basis):\n",
    "                rru = min(ranks[k],rr)\n",
    "                projs.append(np.linalg.norm((np.matmul(c[:rru,:].transpose(),np.matmul(c[:rru,:],x.transpose()))).transpose() - x))\n",
    "\n",
    "            recon_err.append(projs)\n",
    "\n",
    "        errs = np.stack(recon_err)\n",
    "        #return errs,recon_err\n",
    "        err = sum(np.min(errs,1))\n",
    "        #print err\n",
    "\n",
    "        prev_assignment = np.copy(new_assignment)\n",
    "\n",
    "        temp_assignment = np.argmin(errs,1)\n",
    "        sample_order = np.random.permutation(sample_order)\n",
    "        selected_index = sample_order[:int(vocab_size*0.1)]\n",
    "        new_assignment[selected_index] = temp_assignment[selected_index]\n",
    "        #print sum(new_assignment == prev_assignment)\n",
    "        if(sum(new_assignment == prev_assignment) == vocab_size):\n",
    "            break\n",
    "        basis = []\n",
    "        for c in range(n_clusters):\n",
    "            vecs = X_sub[np.where(new_assignment == c)[0]]\n",
    "            weight_for_this_cluster = Weight_sub[np.where(initial_labels == c)[0]]\n",
    "            rank = ranks[c]\n",
    "            B = weighted_svd(vecs,np.log(weight_for_this_cluster)+1e-20,0)\n",
    "            basis.append(B)\n",
    "\n",
    "    clusters = []\n",
    "    sequences = []\n",
    "    final_weights = []\n",
    "    for i in range(n_clusters):\n",
    "        clusters.append(X_sub[np.where(new_assignment == i)[0]])\n",
    "        sequences.append(np.where(new_assignment == i)[0]+entries_to_take)\n",
    "        final_weights.append(Weight_sub[np.where(new_assignment == i)[0]])\n",
    "\n",
    "\n",
    "    approximated_matrice = []\n",
    "    Low_rank_matrice = []\n",
    "\n",
    "\n",
    "    for k,p in enumerate(clusters):\n",
    "        rank = ranks[k]\n",
    "        #rank = max(1,int(parameter_ratio[k] * total_budgets/(p.shape[0] + hidden_size)))\n",
    "        # Don't approximate if the cluster is too small, which is unlikely to happen\n",
    "        if(p.shape[0] < 10):\n",
    "            rank = p.shape[0]\n",
    "\n",
    "        weight_for_this_cluster = final_weights[k]\n",
    "\n",
    "        U,V = weighted_svd(p,np.log(weight_for_this_cluster)+1e-20,maximal_rank)\n",
    "#        new_W1 = np.zeros(U.shape)\n",
    "#        new_W2 = np.zeros(V.shape)\n",
    "#        for i in range(U.shape[0]):\n",
    "#            for j in range(U.shape[1]):\n",
    "#                new_W1[i][j] = my_fp6(binary(U[i][j]))\n",
    "#                new_W1[i][j] = np.float32(np.float\n",
    "#        for i in range(V.shape[0]):\n",
    "#            for j in range(V.shape[1]):\n",
    "#                new_W2[i][j] = my_fp6(binary(V[i][j]))\n",
    "#        U = new_W1\n",
    "#        V = new_W2\n",
    "#        print(np.unique(U))\n",
    "#        print(np.unique(V))\n",
    "#        U = np.float32(np.float16(U))\n",
    "#        V = np.float32(np.float16(V))\n",
    "#        recovery = np.matmul(new_W1,new_W2)\n",
    "        recovery = np.matmul(U,V)\n",
    "        approximated_matrice.append(recovery)\n",
    "        Low_rank_matrice.append([U,V])\n",
    "\n",
    "\n",
    "    new_X = np.zeros(X.shape)\n",
    "\n",
    "    tt = sum(ranks) * hidden_size\n",
    "    for i in range(n_clusters):\n",
    "        print np.where(new_assignment == i)[0].shape[0]\n",
    "    #return sequences,Low_rank_matrice,new_X,entries_to_take\n",
    "    for kk,s in enumerate(sequences):\n",
    "        L_U,L_V = Low_rank_matrice[kk]\n",
    "        print s.shape\n",
    "        #print L_U.shape,L_V.shape\n",
    "        for k,p in enumerate(s):\n",
    "            #new_X[p] = approximated_matrice[kk][k]\n",
    "            rrr = min(ranks_for_each_param[p-entries_to_take],ranks[kk])\n",
    "            #print rrr\n",
    "            tt += rrr\n",
    "            new_X[p] = np.matmul(L_U[k][:rrr],L_V[:rrr,:])\n",
    "    new_X[:entries_to_take,:] = X[:entries_to_take,:]\n",
    "    print tt,np.prod(X.shape),tt/float(np.prod(X.shape))\n",
    "\n",
    "    return new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET FREQ INFORMATION FROM TRAIN DATA\n",
    "vocab_size = 10000\n",
    "a = np.zeros(vocab_size)\n",
    "for t in train_data:\n",
    "    a[t] += 1\n",
    "a = np.asarray(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "270\n",
      "82\n",
      "43\n",
      "27\n",
      "18\n",
      "2858\n",
      "2112\n",
      "1721\n",
      "1544\n",
      "1265\n",
      "(2858,)\n",
      "(2112,)\n",
      "(1721,)\n",
      "(1544,)\n",
      "(1265,)\n",
      "1495623 15000000 0.0997082\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "Rec_W1 = Compress(W1,0.10,a,0.05,5,5)\n",
    "# Rec_W2 = Compress(W2,0.10,a,0.05,5,5)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.91794159056336\n"
     ]
    }
   ],
   "source": [
    "    sess.run(tf.assign(tvars[0],Rec_W1))\n",
    "    sess.run(tf.assign(tvars[-2],W2.transpose()))\n",
    "    # VALIDATE #\n",
    "    stateb = sess.run(initial_state)\n",
    "    t = np.arange(len(x_test_batches))\n",
    "    cc = 0.0\n",
    "    cnt = 0\n",
    "    for k,idx in enumerate(t):\n",
    "        feed_dict = {}\n",
    "        for i, (c, h) in enumerate(initial_state):\n",
    "            feed_dict[c] = stateb[i].c\n",
    "            feed_dict[h] = stateb[i].h\n",
    "        feed_dict[_input] = x_test_batches[idx]\n",
    "        feed_dict[_targets] = y_test_batches[idx]    \n",
    "        _c,stateb = sess.run([val_cost,val_final_state],feed_dict=feed_dict)\n",
    "        cc += _c\n",
    "        cnt += num_steps\n",
    "\n",
    "    print np.exp(cc/cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(open('W1.npy','wb'),W1)\n",
    "np.save(open('freqs','wb'),a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rec_W1_ = np.load(open('W1.zip.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.49799848030283\n"
     ]
    }
   ],
   "source": [
    "    sess.run(tf.assign(tvars[0],Rec_W1_))\n",
    "    sess.run(tf.assign(tvars[-2],W2.transpose()))\n",
    "    # VALIDATE #\n",
    "    stateb = sess.run(initial_state)\n",
    "    t = np.arange(len(x_test_batches))\n",
    "    cc = 0.0\n",
    "    cnt = 0\n",
    "    for k,idx in enumerate(t):\n",
    "        feed_dict = {}\n",
    "        for i, (c, h) in enumerate(initial_state):\n",
    "            feed_dict[c] = stateb[i].c\n",
    "            feed_dict[h] = stateb[i].h\n",
    "        feed_dict[_input] = x_test_batches[idx]\n",
    "        feed_dict[_targets] = y_test_batches[idx]    \n",
    "        _c,stateb = sess.run([val_cost,val_final_state],feed_dict=feed_dict)\n",
    "        cc += _c\n",
    "        cnt += num_steps\n",
    "\n",
    "    print np.exp(cc/cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1500)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p27)",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
